[localhost]
PARALLEL_COMMAND = mpirun -np %_(JOB_NODES)d -bynode %_(COMMAND)s
NAME = SLURM
MANDATORY = False
SUBMIT_COMMAND = salloc -p %_(JOB_QUEUE)s sbatch %_(JOB_SCRIPT)s
CANCEL_COMMAND = scancel %_(JOB_ID)s
CHECK_COMMAND = squeue -j %_(JOB_ID)s
SUBMIT_TEMPLATE = #!/bin/bash
        ### Job name
        #SBATCH -J %_(JOB_NAME)s
        ### Outputs (we need to escape the job id as %%j)
        #SBATCH -o job%%j.out
        #SBATCH -e job%%j.err
        ### Partition (queue) name
        ### if the system has only 1 queue, it can be omited
        ### if you want to specify the queue, ensure the name in the scipion dialog matches
        ### a slurm partition, then leave only 1 # sign in the next line
        #SBATCH -p %_(JOB_QUEUE)s %_(JOB_GPU)s

        ### Specify time, number of nodes (tasks), cores and memory(MB) for your job
        #SBATCH --time=%_(JOB_TIME)s:00:00
        #SBATCH --nodes=1
        #SBATCH --ntasks=%_(JOB_NODES)d
        #SBATCH --cpus-per-task=%_(JOB_THREADS)d
        #SBATCH --mem=%_(JOB_MEMORY)s
        # Use as working dir the path where sbatch was launched
        WORKDIR=$SLURM_JOB_SUBMIT_DIR

        #################################
        ### Set environment varible to know running mode is non interactive
        export XMIPP_IN_QUEUE=1

        cd $WORKDIR
        # Make a copy of node file
        cp $SLURM_JOB_NODELIST %_(JOB_NODEFILE)s
        # Calculate the number of processors allocated to this run.
        NPROCS=`wc -l < $SLURM_JOB_NODELIST`
        # Calculate the number of nodes allocated.
        NNODES=`uniq $SLURM_JOB_NODELIST | wc -l`

        ### Display the job context
        echo Running on host `hostname`
        echo Time is `date`
        echo Working directory is `pwd`
        echo Using ${NPROCS} processors across ${NNODES} nodes
        echo NODE LIST:
        cat $SLURM_JOB_NODELIST
        #################################
        %_(JOB_COMMAND)s
QUEUES = {
        "cm-gpu": [["JOB_MEMORY", "8192", "Memory (MB)", "Select amount of memory (in megabytes) for this job"],
                ["JOB_TIME", "120", "Time (hours)", "Select the time expected (in hours) for this job"],
                ["JOB_GPU", "--gres=gpu:2", "Number of GPUs reserverd for this job"]
               ]
         }
